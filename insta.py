import instaloader
from instaloader import Post
import os
import shutil
import lzma
import json
from datetime import datetime
import subprocess
import time
import traceback
from itertools import islice
import re

USERNAME = ""
COOKIE = {}
COOKIE_FILE_PATH = r''

# æ ‡è®°æ˜¯å¦å·²è®°å½•æ—¶é—´
LOG_TIME_WRITTEN = False
ROOT_DIR = r""  # æ ¹ç›®å½•
FOLLOWING_FILE = ""  # ç”¨æˆ·ååˆ—è¡¨

L = instaloader.Instaloader()  # åˆ›å»º Instaloader å®ä¾‹


def get_instagram_cookies():
    global COOKIE  # å£°æ˜ä¸ºå…¨å±€å˜é‡

    target_cookies = ['sessionid', 'csrftoken', 'ds_user_id', 'mid', 'ig_did']
    cookies_dict = {}

    try:
        with open(COOKIE_FILE_PATH, 'r', encoding='utf-8') as file:
            cookies = file.readlines()

        for line in cookies:
            if line.startswith('#'):
                continue
            parts = line.strip().split('\t')
            if len(parts) >= 7:
                name = parts[5]
                value = parts[6]
                if name in target_cookies:
                    cookies_dict[name] = value

        COOKIE = {
            "sessionid": cookies_dict.get("sessionid", ""),
            "csrftoken": cookies_dict.get("csrftoken", ""),
            "ds_user_id": cookies_dict.get("ds_user_id", ""),
            "mid": cookies_dict.get("mid", ""),
            "ig_did": cookies_dict.get("ig_did", "")
        }

    except FileNotFoundError:
        print(f"[é”™è¯¯] æœªæ‰¾åˆ° cookie æ–‡ä»¶ï¼š{COOKIE_FILE_PATH}")
        COOKIE = {}
    except Exception as e:
        print(f"[é”™è¯¯] å¤„ç† cookie æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        COOKIE = {}


def read_following_usernames(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip()]


def get_all_md_urls_with_shortcode(folder_path):
    results = []
    pattern = re.compile(r'\*\*URLï¼š\*\* \[(https?://www\.instagram\.com/p/([^/]+)/?)\]')

    for filename in os.listdir(folder_path):
        if filename.endswith(".md"):
            md_path = os.path.join(folder_path, filename)
            with open(md_path, "r", encoding="utf-8") as f:
                content = f.read()
                match = pattern.search(content)
                if match:
                    url = match.group(1)
                    shortcode = match.group(2)
                    results.append((shortcode, url))
    return results


def download_reels(username_to_download, base_dir, post_urls, base_dir1, idx_main, total_users_main, max_reels,markdown_dir, times, mode):
    # gallery-dl ä¸‹è½½ Reels
    os.makedirs(base_dir, exist_ok=True)
    COOKIE_FILE_PATH  # ä½ çš„ JSON cookies è·¯å¾„

    start = 1 + (times - 1) * max_reels
    end = max_reels * times

    try:
        command = [
            "gallery-dl",
            f"https://www.instagram.com/{username_to_download}/reels/",
            "--cookies", COOKIE_FILE_PATH,
            "--write-metadata",  # ç”Ÿæˆ .json æ–‡ä»¶
            "--no-download",  # ä¸ä¸‹è½½å®é™…çš„è§†é¢‘æ–‡ä»¶
            "-d", base_dir
        ]
        # å¦‚æœ mode == 2, é™åˆ¶åªä¸‹è½½æŒ‡å®šèŒƒå›´çš„ Reels
        if mode == 2:
            command.append("--range")
            command.append(f"{start}-{end}")
        # æ‰§è¡Œå‘½ä»¤
        subprocess.run(command, check=True)
        print(f"âœ… æˆåŠŸä¸‹è½½ Reels å…ƒæ•°æ®ï¼š{username_to_download}")

    except subprocess.CalledProcessError as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{e}")
        return [], False

    if mode == 2:
        post_urls = get_all_md_urls_with_shortcode(markdown_dir)

    reel_urls = []
    found_duplicate_once = False
    found_duplicate_twice = False

    for file in os.listdir(base_dir1):
        if file.endswith(".json"):
            json_path = os.path.join(base_dir1, file)
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                post_url = data.get("post_url")
                if post_url:
                    shortcode = post_url.rstrip("/").split("/")[-1]
                    # å…ˆåˆ¤æ–­æ˜¯å¦å·²åœ¨ reel_urls ä¸­
                    if any(post_url == url for _, url in reel_urls):
                        continue
                    # å†åˆ¤æ–­æ˜¯å¦åœ¨å·²æœ‰ post_urls ä¸­
                    if any(post_url == url for _, url in post_urls):
                        print(f"âš ï¸ é‡å¤çš„ URLï¼š{post_url}")
                        if times != 1:
                            if found_duplicate_once:
                                found_duplicate_twice = True
                            else:
                                found_duplicate_once = True
                    else:
                        reel_urls.append((shortcode, post_url))
                        print(f"âœ… æ–°çš„ Reels URLï¼š{post_url}")
    print("times", times)
    print("found_duplicate_once:", found_duplicate_once)
    print("found_duplicate_twice:", found_duplicate_twice)
    save_post_urls(username_to_download, reel_urls, "downloaded_reel_urls.txt")

    if mode == 1:
        if not reel_urls:
            print(f"âš ï¸ æ²¡æœ‰æ–° Reels URLï¼Œè·³è¿‡ä¸‹è½½å’Œå¤„ç†ï¼š{username_to_download}")
            return []

    download_user(choose, username_to_download, reel_urls, idx_main, total_users_main)
    log_process(username_to_download, "Instagram")
    return reel_urls, found_duplicate_twice


def rename_reels_files(folder_path, username_to_download):
    base_dir, markdown_dir, media_dir, metadata_dir = path(username_to_download, "Instagram")
    for file in os.listdir(folder_path):
        if file.endswith(".json"):
            json_path = os.path.join(folder_path, file)
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            date_str = data.get("date")
            if not date_str:
                print(f"âŒ ç¼ºå°‘ date å­—æ®µï¼š{file}")
                continue

            try:
                dt = datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S")
                new_base = dt.strftime("%Y-%m-%d_%H-%M-%S_UTC")
            except ValueError:
                print(f"âš ï¸ æ—¶é—´æ ¼å¼é”™è¯¯ï¼š{date_str} in {file}")
                continue

            old_base = file.replace(".json", "")
            mp4_old = os.path.join(folder_path, f"{old_base}")
            json_old = json_path

            mp4_new = os.path.join(folder_path, f"{new_base}.mp4")
            json_new = os.path.join(folder_path, f"{new_base}_reel_gallery-dl.json")

            # ä¸ºé¿å… WinError 32ï¼Œç¨ä½œç­‰å¾…
            time.sleep(0.01)

            if os.path.exists(mp4_old):
                os.rename(mp4_old, mp4_new)
                print(f"mp4  é‡å‘½åï¼š{mp4_old} â†’ {mp4_new}")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶ï¼š{mp4_old}")

            os.rename(json_old, json_new)
            print(f"json é‡å‘½åï¼š{json_old} â†’ {json_new}")

            # âœ… è½¬ç§»æ–‡ä»¶
            metadata_gallery_dl_dir = os.path.join(folder_path, "metadata_gallery-dl")
            os.makedirs(media_dir, exist_ok=True)
            os.makedirs(metadata_gallery_dl_dir, exist_ok=True)

            if os.path.exists(mp4_new):
                shutil.move(mp4_new, os.path.join(media_dir, os.path.basename(mp4_new)))
                print(f"ğŸ“ ç§»åŠ¨è§†é¢‘åˆ° {media_dir}")
            if os.path.exists(json_new):
                shutil.move(json_new, os.path.join(metadata_gallery_dl_dir, os.path.basename(json_new)))
                print(f"ğŸ“ ç§»åŠ¨å…ƒæ•°æ®åˆ° {metadata_gallery_dl_dir}")


def process_user_reels(username_to_download, idx_main, total_users_main, post_urls, times, mode):
    base_dir, markdown_dir, media_dir, metadata_dir = path(username_to_download, None)
    base_dir1, markdown_dir1, media_dir1, metadata_dir1 = path(username_to_download, "Instagram")
    if mode == 1:
        # ä¼ å…¥ post_urls
        download_reels(username_to_download, base_dir, post_urls, base_dir1, idx_main, total_users_main, 262144,
                       markdown_dir1, times, mode=1)
        rename_reels_files(base_dir1, username_to_download)
        return False
    elif mode == 2:
        # ä¼ å…¥ post_urls
        reel_urls, found_duplicate_twice = download_reels(username_to_download, base_dir, post_urls, base_dir1,
                                                          idx_main, total_users_main, 3, markdown_dir1, times, mode=2)
        rename_reels_files(base_dir1, username_to_download)
        return reel_urls, found_duplicate_twice


def path(username_to_download, Insta):
    if Insta == None:
        base_dir = os.path.join(ROOT_DIR)
    else:
        base_dir = os.path.join(ROOT_DIR, Insta, username_to_download)
    markdown_dir = os.path.join(base_dir, "zmarkdown")
    media_dir = os.path.join(base_dir, "media")
    metadata_dir = os.path.join(base_dir, "metadata")
    return base_dir, markdown_dir, media_dir, metadata_dir


def get_all_post_urls(username_to_download, times, mode):
    L.context._session.cookies.update(COOKIE)

    profile = instaloader.Profile.from_username(L.context, username_to_download)
    post_urls = []

    if mode == 1:
        for post in profile.get_posts():
            shortcode = post.shortcode
            url = f"https://www.instagram.com/p/{shortcode}/"
            post_urls.append((shortcode, url))

        print(f"å…±è·å–åˆ° {len(post_urls)} ä¸ª post")
        print(post_urls)
        return post_urls
    elif mode == 2:
        cache = 3
        start = (times - 1) * cache
        end = times * cache
        posts = profile.get_posts()  # å¿…é¡»å…ˆæ‹¿åˆ° posts
        posts = islice(posts, start, end)  # å†æ ¹æ® start/end åˆ‡ç‰‡

        for post in posts:
            shortcode = post.shortcode
            url = f"https://www.instagram.com/p/{shortcode}/"
            post_urls.append((shortcode, url))

        print(f"times={times} è·å–åˆ° {len(post_urls)} ä¸ª post")
        print(post_urls)
        return post_urls


def remove_failed_log_entry(shortcode):
    if not os.path.exists('failed_downloads.txt'):
        return

    with open('failed_downloads.txt', 'r', encoding='utf-8') as fr:
        lines = fr.readlines()

    new_lines = []
    i = 0
    while i < len(lines):
        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„ä¸‰è¡Œè®°å½•
        if i + 2 < len(lines) and shortcode in lines[i + 2]:
            # è·³è¿‡è¿™ä¸‰è¡Œï¼ˆå³åˆ é™¤ï¼‰
            i += 3
        else:
            new_lines.append(lines[i])
            i += 1

    with open('failed_downloads.txt', 'w', encoding='utf-8') as fw:
        fw.writelines(new_lines)


def log_failed_download(username_to_download, shortcode, url, log_file='failed_downloads.txt'):
    global LOG_TIME_WRITTEN
    with open(log_file, 'a', encoding='utf-8') as f:
        # è®°å½•æ—¶é—´
        f.write(f'è®°å½•æ—¶é—´ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
        # å†™å…¥å¤±è´¥çš„ä¸‹è½½ä¿¡æ¯
        f.write(f'username: {username_to_download}\n')
        f.write(f'{shortcode}\t{url}\n')
    print(f"Invalid log file path: {log_file}")


def download_posts_by_url(choose, username_to_download, post_urls, base_dir, idx_main, total_users_main):
    os.makedirs(base_dir, exist_ok=True)
    L.context._session.cookies.update(COOKIE)
    L.dirname_pattern = base_dir

    total_url = len(post_urls)

    for idx, (shortcode, url) in enumerate(post_urls, start=1):
        try:
            post = Post.from_shortcode(L.context, shortcode)
            L.download_post(post, target=f"{shortcode}")
            print(f"âœ… ä¸‹è½½æˆåŠŸï¼š{shortcode}: [{total_users_main}/{idx_main}]: [{total_url}/{idx}]")
            if choose == "2":
                remove_failed_log_entry(shortcode)
        except Exception as e:
            print(f"[{total_url}/{idx}]: âŒ ä¸‹è½½å¤±è´¥ {shortcode}: {e}")
            if not choose == "2":
                log_failed_download(username_to_download, shortcode, url)


def download_user(choose, username_to_download, post_urls, idx_main, total_users_main):
    base_dir, markdown_dir, media_dir, metadata_dir = path(username_to_download, "Instagram")

    os.makedirs(markdown_dir, exist_ok=True)
    os.makedirs(media_dir, exist_ok=True)
    os.makedirs(metadata_dir, exist_ok=True)

    download_posts_by_url(choose, username_to_download, post_urls, base_dir, idx_main, total_users_main)

    # åˆ†ç±»æ•´ç†
    for filename in os.listdir(base_dir):
        file_path = os.path.join(base_dir, filename)
        if os.path.isfile(file_path):
            if filename.endswith((".txt", ".md")):
                shutil.move(file_path, os.path.join(markdown_dir, filename))
            elif filename.endswith((".jpg", ".png", ".mp4", ".webp")):
                shutil.move(file_path, os.path.join(media_dir, filename))
            elif filename.endswith(".json.xz"):
                shutil.move(file_path, os.path.join(metadata_dir, filename))


def log_process(username_to_download, Insta):
    base_dir, markdown_dir, media_dir, metadata_dir = path(username_to_download, "Instagram")
    metadata_files = [f for f in os.listdir(metadata_dir) if f.endswith(".json.xz") and f.startswith("20")]
    total = len(metadata_files)

    for idx, filename in enumerate(metadata_files, start=1):
        print(f"æ­£åœ¨å¤„ç† [{idx}/{total}]: {filename}")
        base_name = filename.replace(".json.xz", "")
        post_time = base_name
        json_xz_path = os.path.join(metadata_dir, filename)
        json_path = os.path.join(metadata_dir, f"{base_name}.json")

        # è§£å‹ json.xz æˆ json
        with lzma.open(json_xz_path) as f_in, open(json_path, "wb") as f_out:
            f_out.write(f_in.read())
        # åˆ é™¤ .xz æ–‡ä»¶
        os.remove(json_xz_path)
        # åŠ è½½è§£å‹åçš„ json æ•°æ®
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        txt_path = os.path.join(markdown_dir, f"{base_name}.txt")
        title = "ï¼ˆæ— æ ‡é¢˜ï¼‰"
        if os.path.exists(txt_path):
            with open(txt_path, "r", encoding="utf-8") as f:
                title = f.read().strip()

        node = data.get("node", {})
        shortcode = node.get("shortcode", "unknown")
        url = f"https://www.instagram.com/p/{shortcode}/"
        likes = node.get("edge_media_preview_like", {}).get("count", 0)
        comment_counts = node.get("edge_media_preview_comment", {}).get("count", 0)

        # post = Post.from_shortcode(L.context, shortcode)
        # æ›¿æ¢ md_lines ä¸­æ·»åŠ è¯„è®ºçš„éƒ¨åˆ†ï¼š
        md_lines = [
            f"# Instagram Post",
            f"**ä¸Šä¼ æ—¶é—´ï¼š** {post_time}",
            f"**æ ‡é¢˜ï¼š** {title}",
            f"**URLï¼š** [{url}]({url})",
            f"**ç‚¹èµæ•°ï¼š** {likes}",
            f"**è¯„è®ºæ•°ï¼š** {comment_counts}",
            # "\n## è¯„è®ºï¼š"
        ]

        # å¤„ç†åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡å’Œè§†é¢‘ï¼‰
        media_files = [f for f in os.listdir(media_dir) if f.endswith((".jpg", ".png", ".mp4", ".webp"))]

        # ä»æ–‡æœ¬æ–‡ä»¶åä¸­æå–æ—¶é—´æˆ³ï¼ˆå»æ‰ .md åç¼€ï¼‰
        base_name_without_extension = base_name  # e.g., "2025-04-03_00-35-57_UTC"
        for media_file in media_files:
            # æå–åª’ä½“æ–‡ä»¶åä¸­çš„æ—¶é—´æˆ³éƒ¨åˆ†
            media_name_without_extension = os.path.splitext(media_file)[0]  # å»æ‰æ–‡ä»¶æ‰©å±•å
            if media_name_without_extension.startswith(base_name_without_extension):
                if media_file.endswith((".jpg", ".png", ".webp")):
                    # æ·»åŠ å›¾ç‰‡çš„ HTML æ ‡ç­¾ï¼Œå¹¶è®¾ç½®å®½åº¦
                    md_lines.append(f'<img src="{os.path.join(media_dir, media_file)}" height="650" />')
                elif media_file.endswith(".mp4"):
                    # æ·»åŠ è§†é¢‘çš„ HTML æ ‡ç­¾ï¼Œå¹¶è®¾ç½®å®½åº¦
                    md_lines.append(f'<video id="video" controls="" preload="none" width="35%" height="auto">')
                    md_lines.append(
                        f'    <source id="mp4" src="{os.path.join(media_dir, media_file)}" type="video/mp4">')
                    md_lines.append(f'</video>')

        # å¤„ç†æ¯ä¸ªè¯„è®º
        '''
        for comment in post.get_comments():
            user = comment.owner.username
            text = comment.text
            created_at = comment.created_at_utc.strftime("%Y-%m-%d %H:%M:%S")
            comment_likes = getattr(comment, "likes_count", 0)
            md_lines.append(f"- **{user}** ({created_at}, likes: {comment_likes}): {text}")

            if comment.answers:
                for reply in comment.answers:
                    reply_user = reply.owner.username
                    reply_text = reply.text
                    reply_created_at = reply.created_at_utc.strftime("%Y-%m-%d %H:%M:%S")
                    reply_likes = getattr(reply, "likes_count", 0)
                    md_lines.append(
                        f"    - â†³ **{reply_user}** ({reply_created_at}, likes: {reply_likes}): {reply_text}"
                    )
        '''

        # å°† md_lines å†™å…¥æ–‡ä»¶
        md_path = os.path.join(markdown_dir, f"{base_name}.md")
        with open(md_path, "w", encoding="utf-8") as f:
            f.write("\n".join(md_lines))

        if os.path.exists(txt_path):
            os.remove(txt_path)


def read_failed_downloads(log_file='failed_downloads.txt'):
    user_post_map = {}

    current_username = None

    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line.startswith('username: '):
                current_username = line.replace('username: ', '').strip()
                if current_username not in user_post_map:
                    user_post_map[current_username] = set()
            elif not line.startswith('è®°å½•æ—¶é—´') and current_username:
                parts = line.split('\t')
                if len(parts) == 2:
                    shortcode, url = parts
                    user_post_map[current_username].add((shortcode.strip(), url.strip()))

    # å±•å¹³ä¸ºåˆ—è¡¨
    usernames = list(user_post_map.keys())
    user_posts = list(user_post_map.values())

    return usernames, user_posts


def save_post_urls(username, post_urls, output_file):
    with open(output_file, 'a', encoding='utf-8') as f:
        f.write(f"username: {username}\n")
        for shortcode, url in post_urls:
            f.write(f"{shortcode}\t{url}\n")


def get_latest_downloaded_filename(username_to_download, idx_main, total_users_main):
    base_dir, markdown_dir, media_dir, metadata_dir = path(username_to_download, "Instagram")
    post_updates = []
    # è·å– metadata_dir ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    existing_files = os.listdir(metadata_dir)
    context = instaloader.InstaloaderContext()

    # posts update
    p_found_duplicate_once = False  # è®°å½•ç¬¬ä¸€æ¬¡é‡å¤
    p_found_duplicate_twice = False  # è®°å½•ç¬¬äºŒæ¬¡é‡å¤
    max_timep = 65536
    stop_download = False  # å¤–å±‚æ§åˆ¶
    for times in range(1, max_timep + 1):
        if stop_download:
            break  # åœ¨æ¯æ¬¡ times å¾ªç¯å¼€å§‹æ—¶æ£€æŸ¥ï¼

        post_urls = get_all_post_urls(username_to_download, times, mode=2)

        for shortcode, url in post_urls:
            post = instaloader.Post.from_shortcode(context, shortcode)
            post_datetime = post.date_utc
            formatted_time = post_datetime.strftime('%Y-%m-%d_%H-%M-%S_UTC')
            file_name = f"{formatted_time}.json"

            if file_name in existing_files:
                print(f"âŒ æ–‡ä»¶é‡å¤ï¼š{file_name} ")

                if p_found_duplicate_once:
                    p_found_duplicate_twice = True
                else:
                    p_found_duplicate_once = True

                if times >= 2 and p_found_duplicate_twice:
                    print(f"âŒ è¿ç»­ä¸¤æ¬¡æ£€æµ‹åˆ°é‡å¤ï¼Œåœæ­¢ä¸‹è½½ï¼times={times}")
                    stop_download = True
                    break  # è·³å‡ºå†…å±‚ for
                else:
                    break  # åªè·³å‡ºå½“å‰ post_urls å¾ªç¯ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡ times
            else:
                print(f"âœ… æ·»åŠ æ–‡ä»¶ï¼š{file_name}")
                post_updates.append((shortcode, url))
                p_found_duplicate_once = False  # æˆåŠŸæ·»åŠ åˆ™æ¸…ç©ºç¬¬ä¸€æ¬¡é‡å¤æ ‡å¿—
        download_user("1", username_to_download, post_updates, idx_main, total_users_main)

    # reels update
    max_timer = 65536
    # æ³¨æ„çœç•¥ç½®é¡¶å¸–æ–‡
    for times in range(1, max_timer + 1):
        reel_urls, found_duplicate_twice = process_user_reels(username_to_download, idx_main, total_users_main,
                                                              post_urls, times=times, mode=2)
        print("found_duplicate", found_duplicate_twice)
        if times >= 2 and found_duplicate_twice:
            print(f"è¿ç»­ä¸¤æ¬¡æ£€æµ‹åˆ°é‡å¤ï¼Œåœæ­¢ï¼štimes={times}")
            break


if __name__ == "__main__":
    get_instagram_cookies()
    choose = input("Choose: 1.download 2.redownload 3.update")
    if choose == "1":
        username_to_download = read_following_usernames(FOLLOWING_FILE)
        total_users_main = len(username_to_download)
        print(f"å…±è¯»å– {total_users_main} ä¸ªç”¨æˆ·å")

        for idx_main, username in enumerate(username_to_download, start=1):
            print(f"[{idx_main}/{total_users_main}] æ­£åœ¨å¤„ç†ç”¨æˆ·ï¼š{username}")
            post_urls = get_all_post_urls(username, times=-1, mode=1)
            save_post_urls(username, post_urls, "downloaded_post_urls.txt")
            download_user(choose, username, post_urls, idx_main, total_users_main)
            log_process(username, "Instagram")
            process_user_reels(username, idx_main, total_users_main, post_urls, times=-1, mode=1)
    elif choose == "2":
        username_to_download, user_posts_list = read_failed_downloads()
        total_users_main = len(username_to_download)
        print(f"å…±è¯»å– {total_users_main} ä¸ªç”¨æˆ·å")

        for idx_main, (username, post_urls) in enumerate(zip(username_to_download, user_posts_list), 1):
            post_urls = list(post_urls)  # set è½¬ list
            print(f"[{idx_main}/{total_users_main}] æ­£åœ¨å¤„ç†ç”¨æˆ·ï¼š{username}")
            download_user(choose, username, post_urls, idx_main, total_users_main)
            log_process(username, "Instagram")
    elif choose == "3":
        username_to_download = read_following_usernames('file_list.txt')
        total_users_main = len(username_to_download)
        print(f"å…±è¯»å– {total_users_main} ä¸ªç”¨æˆ·å")

        for idx_main, username in enumerate(username_to_download, start=1):
            print(f"[{idx_main}/{total_users_main}] æ­£åœ¨å¤„ç†ç”¨æˆ·ï¼š{username}")
            get_latest_downloaded_filename(username, idx_main, total_users_main)